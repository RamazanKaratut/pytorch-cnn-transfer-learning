# GeliÅŸmiÅŸ CNN TasarÄ±mÄ± ve Transfer Learning (ResNet50)

Bu proje, CIFAR-10 veri seti Ã¼zerinde kendi Custom CNN (EvriÅŸimli Sinir AÄŸÄ±) mimarimizi sÄ±fÄ±rdan oluÅŸturmayÄ± ve endÃ¼stri standardÄ± olan **Transfer Learning** yaklaÅŸÄ±mlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rmayÄ± amaÃ§lamaktadÄ±r.

## ğŸ§  GÃ¶rev 1: Custom CNN TasarÄ±mÄ±
SÄ±fÄ±rdan bir CNN mimarisi tasarlanmÄ±ÅŸ ve **>%85 Test BaÅŸarÄ±sÄ±** hedefine ulaÅŸmak iÃ§in aÅŸaÄŸÄ±daki teknikler uygulanmÄ±ÅŸtÄ±r:
* **3x EvriÅŸim BloÄŸu (Conv Block):** Her blokta birbirini takip eden 2 adet `Conv2d` katmanÄ± ve ardÄ±ndan boyut dÃ¼ÅŸÃ¼rme iÃ§in `MaxPool2d` kullanÄ±lmÄ±ÅŸtÄ±r. AÄŸ giderek derinleÅŸen filtre yapÄ±larÄ±na (32 -> 64 -> 128) sahiptir.
* **Batch Normalization:** Her evriÅŸim iÅŸleminden sonra eklenerek "Internal Covariate Shift" Ã¶nlenmiÅŸ, aÄŸÄ±n Ã§ok daha hÄ±zlÄ± ve stabil yakÄ±nsamasÄ± (converge) saÄŸlanmÄ±ÅŸtÄ±r.
* **Dropout (AÅŸamalÄ±):** Ä°lk bloklarda %20 ile baÅŸlayan ve Classifier kÄ±smÄ±nda %50'ye kadar Ã§Ä±kan Dropout oranlarÄ± sayesinde modelin veriyi ezberlemesi (overfitting) baÅŸarÄ±lÄ± bir ÅŸekilde Ã¶nlenmiÅŸtir.
* **SonuÃ§:** Modelimiz hedeflenen %85 barajÄ±nÄ± rahatlÄ±kla aÅŸarak eÄŸitim sonunda **%92.41 Test BaÅŸarÄ±sÄ±** elde etmiÅŸtir. AÅŸaÄŸÄ±daki grafikte de gÃ¶rÃ¼leceÄŸi Ã¼zere model, istikrarlÄ± bir ÅŸekilde Ã¶ÄŸrenerek mÃ¼kemmel bir genelleme (generalization) performansÄ± sergilemiÅŸtir.

![Custom CNN SonuÃ§](hw1_custom_cnn_result.png)

## ğŸš€ GÃ¶rev 2: Transfer Learning (ResNet50)
Transfer Learning, bÃ¼yÃ¼k veri setlerinde (ImageNet) eÄŸitilmiÅŸ devasa modellerin bilgi birikimini alÄ±p kendi problemimize uygulamaktÄ±r. Bu projede VGG16/ResNet50 mimarileri arasÄ±ndan daha modern ve verimli olan **ResNet50** seÃ§ilmiÅŸ ve iki farklÄ± strateji test edilmiÅŸtir:

1. **Feature Extraction (Ã–zellik Ã‡Ä±karÄ±mÄ±):** * AÄŸÄ±n mevcut tÃ¼m evriÅŸim katmanlarÄ± dondurulmuÅŸtur (`requires_grad = False`).
   * Sadece eklenen son sÄ±nÄ±flandÄ±rma katmanÄ± (`fc`) CIFAR-10 iÃ§in eÄŸitilmiÅŸtir.
   * *Neden?* Model, ImageNet'ten Ã¶ÄŸrendiÄŸi kenar, kÃ¶ÅŸe, doku gibi genel Ã¶zellikleri bizim verimiz Ã¼zerinde doÄŸrudan bir "filtre" olarak kullanÄ±r. EÄŸitim Ã§ok hÄ±zlÄ±dÄ±r Ã§Ã¼nkÃ¼ geriye yayÄ±lÄ±mda parametreler gÃ¼ncellenmez.

2. **Full Fine-Tuning (Ä°nce Ayar):**
   * AÄŸÄ±n tamamÄ± eÄŸitime aÃ§Ä±k bÄ±rakÄ±lmÄ±ÅŸ ancak Ã¶nceden Ã¶ÄŸrenilmiÅŸ aÄŸÄ±rlÄ±klarÄ± "bozmamak" adÄ±na Ã§ok dÃ¼ÅŸÃ¼k bir Learning Rate (`1e-4`) kullanÄ±lmÄ±ÅŸtÄ±r.
   * *Neden?* Model sadece son katmanÄ± deÄŸil, kendi evriÅŸim filtrelerini de uÃ§ak, kedi, kÃ¶pek (CIFAR-10) gÃ¶rsellerine spesifik olacak ÅŸekilde ufak ufak gÃ¼nceller. Genellikle en yÃ¼ksek baÅŸarÄ±yÄ± bu yÃ¶ntem verir ancak yÃ¼ksek iÅŸlem gÃ¼cÃ¼ gerektirir.

*(Transfer Learning grafiÄŸi yakÄ±nda eklenecektir)*

## ğŸ’» Kurulum ve Ã‡alÄ±ÅŸtÄ±rma
Projeyi Ã§alÄ±ÅŸtÄ±rmak iÃ§in Ã¶ncelikle gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin:
```bash
pip install -r requirements.txt